{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d71b270-853e-43ef-87e2-b68a1b2b0310",
   "metadata": {
    "tags": []
   },
   "source": [
    "# requirements\n",
    "```\n",
    "osmium version 1.13.2\n",
    "libosmium version 2.17.1\n",
    "Supported PBF compression types: none zlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5279bb-4439-44d4-a1c6-28ce2deb02b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this paper, we use New York pbf data ver.2022-02-24T21:21:27Z.\n",
    "# !wget https://download.geofabrik.de/north-america/us/new-york-latest.osm.pbf -O ../data/new-york-latest.osm.pbf\n",
    "# !osmium extract --overwrite --bbox -74.02,40.70,-73.9,40.84 -o ../data/newyork.osm.pbf ../data/new-york-latest.osm.pbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb3d886-c10d-4a28-90ef-386a76a4ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from pathlib import Path\n",
    "pwd = Path(os.getcwd())\n",
    "\n",
    "import pytz\n",
    "import random\n",
    "import sys\n",
    "sys.path.append(str(pwd.parent))\n",
    "sys.path.append(str(pwd.parent / \"config\"))\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from osmread import parse_file, Node, Way\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm\n",
    "\n",
    "from setting import LOCAL_REGION_BOUND\n",
    "from util import haversine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abff767-0437-433a-9c29-4247c6ee5ff3",
   "metadata": {},
   "source": [
    "# Create whole node dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede58a86-9b14-413a-9f35-09f009f0522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/newyork.osm.pbf\"\n",
    "file = parse_file(file_path)\n",
    "\n",
    "nodes = []\n",
    "ways = []\n",
    "for data in tqdm(file):\n",
    "    if isinstance(data, Node):\n",
    "        if data.tags == {}:\n",
    "            nodes.append(data)\n",
    "    if isinstance(data, Way):\n",
    "        if data.nodes[0] != data.nodes[-1]:\n",
    "            ways.append(data)\n",
    "\n",
    "node_df = pd.DataFrame(columns=[\"NodeID\", \"WayID\"])\n",
    "for way in tqdm(ways):\n",
    "    tmp_df = pd.DataFrame({\"NodeID\": way.nodes, \"WayID\": way.id})\n",
    "    node_df = pd.concat([node_df, tmp_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844507cf-b3a1-4fdb-a79a-057ba659c147",
   "metadata": {},
   "source": [
    "# Trim node dataframe with LOCAL_REGION_BOUND."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dda2c1-bc2c-4e7c-b9d1-08a5b6585ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_map_lat = {}\n",
    "node_map_lon = {}\n",
    "for node in nodes:\n",
    "    node_map_lat.update({node.id: node.lat})\n",
    "    node_map_lon.update({node.id: node.lon})\n",
    "node_df[\"Longitude\"] = node_df[\"NodeID\"].map(node_map_lon)\n",
    "node_df[\"Latitude\"] = node_df[\"NodeID\"].map(node_map_lat)\n",
    "\n",
    "node_df = node_df[\n",
    "    (node_df[\"Longitude\"] > LOCAL_REGION_BOUND.west_bound)\n",
    "    & (node_df[\"Longitude\"] < LOCAL_REGION_BOUND.east_bound)\n",
    "    & (node_df[\"Latitude\"] > LOCAL_REGION_BOUND.south_bound)\n",
    "    & (node_df[\"Latitude\"] < LOCAL_REGION_BOUND.north_bound)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8e1bcc-7d5f-4b66-8e29-342398dd3fd7",
   "metadata": {},
   "source": [
    "# create node connection information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae59a9c-0b68-4e1f-8e4d-3a2bcd4138a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_node_df = pd.DataFrame(columns=[\"NodeID\", \"NextNodeID\", \"WayID\", \"Longitude\", \"Latitude\"])\n",
    "connected_node_df_list = []\n",
    "for way_id in tqdm(node_df[\"WayID\"].drop_duplicates()):\n",
    "    tmp_df = node_df[node_df[\"WayID\"]==way_id]\n",
    "    if len(tmp_df) > 1:\n",
    "        next_node_list = tmp_df[\"NodeID\"].values.tolist()[1:] + [-1]\n",
    "    else:\n",
    "        next_node_list = [-1]\n",
    "\n",
    "    assert len(tmp_df) == len(next_node_list)\n",
    "    tmp_df[\"NextNodeID\"] = next_node_list\n",
    "    tmp_df[\"NextNodeID\"] = tmp_df[\"NextNodeID\"].astype(int)\n",
    "    connected_node_df_list.append(tmp_df)\n",
    "connected_node_df = pd.concat(connected_node_df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2330de04-57a7-427d-8623-e9f8bbd5dffa",
   "metadata": {},
   "source": [
    "# extract biggest graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41485c9f-0a1a-4332-9e4c-709c4127b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.Graph()\n",
    "graph.add_nodes_from(connected_node_df[\"NodeID\"].drop_duplicates().values)\n",
    "\n",
    "for from_node, to_node in zip(connected_node_df[\"NodeID\"].values, connected_node_df[\"NextNodeID\"].values):\n",
    "    if to_node == -1:\n",
    "        continue\n",
    "    else:\n",
    "        graph.add_edge(from_node, to_node)\n",
    "\n",
    "max_nodes = max(nx.connected_components(graph), key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6edee1d-9408-42a0-911d-3577c3fa3780",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_node_df = pd.DataFrame(columns=[\"NodeID\", \"NextNodeID\", \"WayID\"])\n",
    "\n",
    "for way_id in tqdm(connected_node_df[\"WayID\"].drop_duplicates()):\n",
    "    node_list = connected_node_df[connected_node_df[\"WayID\"]==way_id][\"NodeID\"].values\n",
    "    filtered_node_list = []\n",
    "    for node_id in node_list:\n",
    "        if node_id in max_nodes:\n",
    "            filtered_node_list.append(node_id)\n",
    "    if len(filtered_node_list) <= 1:\n",
    "        continue\n",
    "    next_node_list = filtered_node_list[1:] + [-1]\n",
    "    assert len(filtered_node_list) == len(next_node_list)\n",
    "\n",
    "    tmp_df = pd.DataFrame({\"NodeID\": filtered_node_list, \"NextNodeID\": next_node_list, \"WayID\": way_id})\n",
    "    tmp_df[\"NextNodeID\"] = tmp_df[\"NextNodeID\"].astype(int)\n",
    "    filtered_node_df = pd.concat([filtered_node_df, tmp_df])\n",
    "\n",
    "filtered_node_df[\"Longitude\"] = filtered_node_df[\"NodeID\"].map(node_map_lon)\n",
    "filtered_node_df[\"Latitude\"] = filtered_node_df[\"NodeID\"].map(node_map_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92d201-b3ff-4f89-87d8-e140f6952be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_info_df = filtered_node_df[[\"NodeID\", \"Longitude\", \"Latitude\"]].drop_duplicates()\n",
    "\n",
    "node_info = {\n",
    "    node_id: {\n",
    "        \"node_index\": node_index,\n",
    "        \"longitude\": longitude,\n",
    "        \"latitude\": lattitude,\n",
    "    } for node_index, (node_id, longitude, lattitude) in enumerate(\n",
    "        zip(node_info_df[\"NodeID\"].values, node_info_df[\"Longitude\"].values, node_info_df[\"Latitude\"].values))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e868e46b-2078-4bfb-9e47-66e43bbef0ba",
   "metadata": {},
   "source": [
    "# check map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc4f5c9-250f-42ea-8b6c-7508d02a6ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=[30,30])\n",
    "plt.scatter(node_info_df[\"Longitude\"], node_info_df[\"Latitude\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5abb5f1-5882-428b-bce1-a524d5b7d09a",
   "metadata": {},
   "source": [
    "# create cost csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806c9859-1eb5-4920-bcd2-3d4fb81826c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = len(node_info_df)\n",
    "adj_matrix = np.ones([num_nodes, num_nodes]) / np.zeros([num_nodes, num_nodes])\n",
    "for i in range(num_nodes):\n",
    "    adj_matrix[i][i] = 0\n",
    "\n",
    "for from_node_id, to_node_id in zip(filtered_node_df[\"NodeID\"].values, filtered_node_df[\"NextNodeID\"].values):\n",
    "    if to_node_id == -1:\n",
    "        continue\n",
    "    from_node_idx = node_info[from_node_id][\"node_index\"]\n",
    "    to_node_idx = node_info[to_node_id][\"node_index\"]\n",
    "\n",
    "    cost = haversine(\n",
    "        node_info[from_node_id][\"longitude\"],\n",
    "        node_info[from_node_id][\"latitude\"],\n",
    "        node_info[to_node_id][\"longitude\"],\n",
    "        node_info[to_node_id][\"latitude\"],\n",
    "    ) / 15 * 60 # Minutes to arrival. Cars drives at 15 kilometers per hour.\n",
    "\n",
    "    adj_matrix[from_node_idx][to_node_idx] = cost\n",
    "    adj_matrix[to_node_idx][from_node_idx] = cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b16f18-d72b-43e5-ab45-c0346b032411",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in tqdm(range(num_nodes)):\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if adj_matrix[i][k]!=np.inf and adj_matrix[k][j]!=np.inf:\n",
    "                adj_matrix[i][j] = min(adj_matrix[i][j], adj_matrix[i][k] + adj_matrix[k][j])\n",
    "adj_matrix_df = pd.DataFrame(adj_matrix)\n",
    "adj_matrix_df.to_csv(\"../data/AccurateMap.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3289e49-1918-4460-b9c9-98e4359e1cdc",
   "metadata": {},
   "source": [
    "# create node csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fbb787-4f3f-47f3-a85b-c38f51f271b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_data_df = filtered_node_df.reset_index(drop=True)[[\"NodeID\", \"Longitude\", \"Latitude\"]]\n",
    "node_data_df.drop_duplicates(inplace=True)\n",
    "node_data_df[[\"RoadName\", \"Gid\", \"Distance\", \"WayID\"]] = np.nan\n",
    "node_data_df[[\"NodeID\", \"WayID\", \"Longitude\", \"Latitude\", \"RoadName\", \"Gid\", \"Distance\"]].to_csv(\"../data/Node.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb04316-4ab2-4ef1-9daa-94af0d981e08",
   "metadata": {},
   "source": [
    "# create node id list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4098c8b-d3fe-4e11-a875-dbd0c344a27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/NodeIDList.txt\", mode=\"w\") as f:\n",
    "    for node_id in set(node_data_df[\"NodeID\"]):\n",
    "        f.write(f\"{node_id}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3667fdea-ad56-47dd-91bb-2cda5cf8e12d",
   "metadata": {},
   "source": [
    "# create driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c1892-2895-47fa-8315-658a0d59434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_start_points = random.choices(node_data_df[\"NodeID\"].values, k=15000)\n",
    "df = pd.DataFrame({\"DriverID\": range(0,15000), \"NodeS\": driver_start_points})\n",
    "df.to_csv(\"../data/Drivers0601.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addcc999-06cf-45ee-849f-5f2a9a01a383",
   "metadata": {},
   "source": [
    "# create order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a47e88-bbc2-418f-a526-7b13c65f6151",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df = pd.read_csv(\"../data/Order/original/yellow_tripdata_2016-06.csv\")\n",
    "\n",
    "USE_COLUMNS = [\n",
    "    \"tpep_pickup_datetime\",\n",
    "    \"tpep_dropoff_datetime\",\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_latitude\",\n",
    "    \"dropoff_longitude\",\n",
    "    \"dropoff_latitude\",\n",
    "]\n",
    "RENAME_DICT = {\n",
    "    \"tpep_pickup_datetime\": \"Start_time\",\n",
    "    \"tpep_dropoff_datetime\": \"End_time\",\n",
    "    \"pickup_longitude\": \"PointS_Longitude\",\n",
    "    \"pickup_latitude\": \"PointS_Latitude\",\n",
    "    \"dropoff_longitude\": \"PointE_Longitude\",\n",
    "    \"dropoff_latitude\": \"PointE_Latitude\",\n",
    "}\n",
    "\n",
    "rename_order_df = order_df[USE_COLUMNS].rename(columns=RENAME_DICT).sort_values(\"Start_time\").reset_index(drop=True)\n",
    "\n",
    "# NewYorkエリア外のレコードを除去する.\n",
    "rename_order_without_outlier_df = rename_order_df[\n",
    "    (rename_order_df[\"PointS_Longitude\"] > LOCAL_REGION_BOUND.west_bound)\n",
    "    & (rename_order_df[\"PointS_Longitude\"] < LOCAL_REGION_BOUND.east_bound)\n",
    "    & (rename_order_df[\"PointE_Longitude\"] > LOCAL_REGION_BOUND.west_bound)\n",
    "    & (rename_order_df[\"PointE_Longitude\"] < LOCAL_REGION_BOUND.east_bound)\n",
    "    & (rename_order_df[\"PointS_Latitude\"] > LOCAL_REGION_BOUND.south_bound)\n",
    "    & (rename_order_df[\"PointS_Latitude\"] < LOCAL_REGION_BOUND.north_bound)\n",
    "    & (rename_order_df[\"PointE_Latitude\"] > LOCAL_REGION_BOUND.south_bound)\n",
    "    & (rename_order_df[\"PointE_Latitude\"] < LOCAL_REGION_BOUND.north_bound)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7184488-deec-4aef-84da-09fb158ac72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NearestNeighbors(algorithm='ball_tree')\n",
    "nn.fit(node_data_df[[\"Longitude\", \"Latitude\"]].values)\n",
    "\n",
    "node_id_list = node_data_df[\"NodeID\"].values.tolist()\n",
    "\n",
    "_, start_indices = nn.kneighbors(\n",
    "    rename_order_without_outlier_df[[\"PointS_Longitude\", \"PointS_Latitude\"]], n_neighbors=1)\n",
    "rename_order_without_outlier_df[\"NodeS\"] = [node_id_list[i[0]] for i in start_indices]\n",
    "\n",
    "_, end_indices = nn.kneighbors(\n",
    "    rename_order_without_outlier_df[[\"PointE_Longitude\", \"PointE_Latitude\"]], n_neighbors=1\n",
    ")\n",
    "rename_order_without_outlier_df[\"NodeE\"] = [node_id_list[i[0]] for i in end_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242b980a-aa7d-42e4-9daa-81248080dac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newyork_datetime_to_utc(datetime_str: str) -> int:\n",
    "    eastern = pytz.timezone('US/Eastern')\n",
    "    date = datetime.strptime(datetime_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    date.astimezone(eastern)\n",
    "    return int(date.timestamp())\n",
    "\n",
    "rename_order_without_outlier_df[\"Start_datetime\"] = rename_order_without_outlier_df[\"Start_time\"].apply(\n",
    "    lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")\n",
    ")\n",
    "rename_order_without_outlier_df[\"ID\"] = range(len(rename_order_without_outlier_df))\n",
    "start_date = datetime(year=2016, month=6, day=1)\n",
    "directory = \"train\"\n",
    "while True:\n",
    "    next_date = start_date + timedelta(days=1)\n",
    "    tmp_df = rename_order_without_outlier_df[\n",
    "        (rename_order_without_outlier_df[\"Start_datetime\"]<next_date)\n",
    "        & (rename_order_without_outlier_df[\"Start_datetime\"]>start_date)\n",
    "    ]\n",
    "    tmp_df[\"Start_time\"] = tmp_df[\"Start_time\"].apply(newyork_datetime_to_utc)\n",
    "    tmp_df[\"End_time\"] = tmp_df[\"End_time\"].apply(newyork_datetime_to_utc)\n",
    "    if start_date.day >= 24:\n",
    "        directory = \"test\"\n",
    "    tmp_df[[\"ID\"] + list(RENAME_DICT.values()) + [\"NodeS\", \"NodeE\"]].to_csv(\n",
    "        f\"../data/Order/modified/{directory}/order_2016{str(start_date.month).zfill(2)}{str(start_date.day).zfill(2)}.csv\",\n",
    "        index=False\n",
    "    )\n",
    "    start_date = next_date\n",
    "    if start_date.month != 6:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23297191-2180-46b3-814c-229595001fac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
